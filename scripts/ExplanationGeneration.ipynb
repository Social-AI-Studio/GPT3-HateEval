{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"https://github.com/kes76963/myproject/blob/main/gpt3_edit.ipynb","timestamp":1665576216973}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install openai\n","!pip install --upgrade openai\n","import openai\n","import pandas as pd\n","import random\n","from collections import defaultdict\n","from tabulate import tabulate\n","openai.api_key = \"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\""],"metadata":{"id":"nXClNiwJwouy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1684120868526,"user_tz":-480,"elapsed":23441,"user":{"displayName":"Ê±™Ê∂µ","userId":"01721555802280632406"}},"outputId":"37e389a6-38a0-47dc-b8d9-cbb68d64fcb2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting openai\n","  Downloading openai-0.27.6-py3-none-any.whl (71 kB)\n","\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.27.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.65.0)\n","Collecting aiohttp (from openai)\n","  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n","\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n","Collecting multidict<7.0,>=4.5 (from aiohttp->openai)\n","  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n","\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->openai)\n","  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n","Collecting yarl<2.0,>=1.0 (from aiohttp->openai)\n","  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n","\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp->openai)\n","  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n","\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting aiosignal>=1.1.2 (from aiohttp->openai)\n","  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n","Installing collected packages: multidict, frozenlist, async-timeout, yarl, aiosignal, aiohttp, openai\n","Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 frozenlist-1.3.3 multidict-6.0.4 openai-0.27.6 yarl-1.9.2\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (0.27.6)\n","Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.27.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.65.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.4)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.2)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.3)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n"]}]},{"cell_type":"markdown","source":["# Sample tweets for evaluation and demostration\n"],"metadata":{"id":"Dc1ffCYgaNap"}},{"cell_type":"code","source":["# from google.colab import drive\n","# drive.mount('/content/gdrive')\n","\n","# root = \"gdrive/MyDrive/PlushProject/GPT3_explanation_evaluation/\"\n","root = \"../\"\n","dataset_path = root + \"data/\"\n","\n","root = \"gdrive/MyDrive/PlushProject/GPT3_explanation_evaluation/demo/\"\n","evaluation_tweets_path = root + \"explanations/evaluation_tweets.csv\"\n","demostration_tweets_path = root + \"explanations/demostration_tweets.csv\"\n","\n","demostration_example_path = root + \"explanations/demonstration_examples/\"\n","evaluation_explanation_path = root + \"explanations/evaluation_explanations/\""],"metadata":{"id":"yPXkOohc3meY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def write_to_file(dic, key_list, sample_size, file_path):\n","    file_list = [[str(i * sample_size + j), key, dic[key][j]] for i, key in enumerate(key_list) for j in range(sample_size)]\n","    df = pd.DataFrame(file_list)\n","    df.to_csv(file_path, header=[\"ID\", \"Type\", \"Tweet\"])\n","\n","# Filter out tweets with words less than 20\n","def filter_tweets(data):\n","    filtered_data = []\n","    for tweet in data:\n","        if len(tweet.split(\" \")) <= 20:\n","            filtered_data.append(tweet)\n","    return filtered_data\n","\n","# Collect a set of 100 tweets for evaluation purposes and gather an additional 20 tweets to form a candidate pool for constructing demonstration examples.\n","def tweets_sampling(dataset_path, sampled_path, demo_example_path, sample_size=25, demo_example_size=5):\n","    sample_pool = defaultdict(list)\n","    demonstration_pool = defaultdict(list)\n","    hate_types = [\"NormalNormalNormal\", \"NormalNormalHate\", \"NormalHateHate\", \"HateHateHate\"]\n","\n","    for hate_type in hate_types:\n","        path = f\"{dataset_path}/{hate_type}.csv\"\n","        data = pd.read_csv(path)\n","        data = [tweet for tweet in data[\"Tweet\"]]\n","\n","        filtered_data = filter_tweets(data)\n","\n","        index_list = random.sample(range(len(filtered_data)), sample_size + demo_example_size)\n","        for index in index_list:\n","            sample_pool[hate_type].append(filtered_data[index])\n","\n","    for hate_type in hate_types:\n","        index_list = random.sample(range(sample_size + demo_example_size), demo_example_size)\n","        for index in index_list:\n","            demonstration_pool[hate_type].append(sample_pool[hate_type][index])\n","        for tweet in demonstration_pool[hate_type]:\n","            sample_pool[hate_type].remove(tweet)\n","\n","    write_to_file(sample_pool, hate_types, sample_size, sampled_path)\n","    write_to_file(demonstration_pool, hate_types, demo_example_size, demo_example_path)\n","\n","def fetch_data(sampled_path):\n","  samplePool = defaultdict(list)\n","  data = pd.read_csv(sampled_path)\n","  for i in range(len(data[\"ID\"])):\n","    tweet = data[\"Tweet\"][i]\n","    tweet_type = data[\"Type\"][i]\n","    samplePool[tweet_type].append(tweet)\n","  return samplePool\n"],"metadata":{"id":"AZUltF40zo4p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# tweets_sampling(dataset_path, evaluation_tweets_path, demostration_tweets_path)\n","\n","samplePool = fetch_data(evaluation_tweets_path)\n","demonstration_pool = fetch_data(demostration_tweets_path)\n","samplePool[\"NormalNormalNormal\"][0]"],"metadata":{"id":"rxk7g6aKl6uM","colab":{"base_uri":"https://localhost:8080/","height":37},"executionInfo":{"status":"ok","timestamp":1684122162120,"user_tz":-480,"elapsed":9,"user":{"displayName":"Ê±™Ê∂µ","userId":"01721555802280632406"}},"outputId":"5e36c658-7d0f-418b-e107-2254d70e97bd"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'<user> bitch i hate you üòÇ üòÇ üòÇ üòÇ'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":20}]},{"cell_type":"markdown","source":["# Manual selection of demonstration examples"],"metadata":{"id":"CtyRJeAw2-5-"}},{"cell_type":"code","source":["Question_dic = {\n","\"hateful-WHY\": \"Question: Please explain why this tweet is hateful? \\n Answer: \",\n","\"non-hateful-WHY\": \"Question: Please explain why this tweet is non-hateful? \\n Answer: \",\n","\"hateful-COT\": \"Question: Please explain why this tweet is hateful? \\n Answer: Let's think step by step. \",\n","\"non-hateful-COT\": \"Question: Please explain why this tweet is non-hateful? \\n Answer: Let's think step by step. \",\n","\"CONTEXT\": \"Question: What is the context of the tweet? \\n Answer: \"\n","}\n","\n","hateTypes = [\"NormalNormalNormal\",\"NormalNormalHate\", \"NormalHateHate\", \"HateHateHate\"]"],"metadata":{"id":"7mu_IBwWnmSv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Generate explanations for demonstration tweets with zero-shot prompting\n","def zero_shot_exp_generation(tweets_dic, question):\n","    exp_list = []\n","    for hateType in hateTypes:\n","        for speech in tweets_dic[hateType]:\n","            prompt = speech + '\\n' + question\n","            run = openai.Completion.create(\n","                model=\"text-davinci-002\",\n","                prompt=prompt,\n","                temperature=0.6,\n","                max_tokens=1000,\n","                n=1,\n","            )\n","            exp = run.choices[0]['text']\n","            exp_list.append([hateType, speech, exp])\n","    return exp_list\n","\n","# Denerate explanations for each demonstration tweets and store them into files.\n","def store_sampled_demo_twt_exp(tweets_dic, question, file_path):\n","    exp_list = zero_shot_exp_generation(tweets_dic, question)\n","    exp_list = [[i] + exp for i, exp in enumerate(exp_list)]\n","    df = pd.DataFrame(exp_list, columns=[\"ID\", \"Type\", \"Tweet\", \"Explanation\"])\n","    df.to_csv(file_path, index=False)\n","\n","# Select demonstration examples based on their explanations' quality\n","def store_selected_demo_twt_exp(selected_ID_list, sampled_path, selected_path):\n","    data = pd.read_csv(sampled_path)\n","    selected_list = []\n","    for id in selected_ID_list:\n","        twt = data.loc[id, \"Tweet\"]\n","        typ = data.loc[id, \"Type\"]\n","        exp = data.loc[id, \"Explanation\"]\n","        selected_list.append([id, typ, twt, exp])\n","    df = pd.DataFrame(selected_list, columns=[\"ID\", \"Type\", \"Tweet\", \"Explanation\"])\n","    df.to_csv(selected_path, index=False)\n"],"metadata":{"id":"Ubnx5wYHo-o0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for key in Question_dic:\n","  store_sampled_demo_twt_exp(demonstration_pool, Question_dic[key], demostration_example_path+ key + \"_demonstration_examples.csv\")\n"],"metadata":{"id":"tgJbi7wqsr88"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# selected_ID_dic =  { \"hateful-WHY\": [1, 2, 3, 4,], \"non-hateful-WHY\": [1, 2, 3, 4,],\n","# \"hateful-COT\": [1, 2, 3, 4,], \"non-hateful-COT\": [1, 2, 3, 4,],\n","# \"CONTEXT\": [1, 2, 3, 4,], }\n","selected_ID_dic =  { \"hateful-WHY\": [], \"non-hateful-WHY\": [],\n","\"hateful-COT\": [], \"non-hateful-COT\": [],\n","\"CONTEXT\": [], }\n","\n","for key in selected_ID_dic:\n","  sampled_demostration_example_path = demostration_example_path + key + \"_demonstration_examples.csv\"\n","  selected_demostration_example_path = demostration_example_path + key + \"_demonstration_examples_selected.csv\"\n","  store_selected_demo_twt_exp(selected_ID_dic[key], sampled_demostration_example_path, selected_demostration_example_path)\n"],"metadata":{"id":"iCj7TI8duI7D"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Explanation generation"],"metadata":{"id":"5QNmevEZwr0w"}},{"cell_type":"code","source":["# Rearrange the demonstration examples by considering the variance in their level of hatefulness compared to the current tweet.\n","# Ensure that the tweets that are closer to the end of the demonstration exhibit hatefulness levels that are more similar to the hatefulness of the current tweet.\n","def reOrder(lis, label):\n","    orders = {\n","        \"NormalNormalNormal\": [0, 1, 2, 3],\n","        \"NormalNormalHate\": [1, 0, 2, 3],\n","        \"NormalHateHate\": [2, 1, 3, 0],\n","        \"HateHateHate\": [3, 2, 1, 0]\n","    }\n","    order = orders.get(label)\n","    if order is None:\n","        return lis\n","    return [lis[i] for i in order]"],"metadata":{"id":"3vQr39VPWgHc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Generate explanaitons by few-shot prompting using four demonstration examples.\n","def few_shots_exp_generation(tweets_dic, demo_list, question, file_path):\n","  exp_lis = []\n","  for hateType in hateTypes:\n","    for speech in tweets_dic[hateType]:\n","      reOrderPromptSample = reOrder(demo_list, hateType)\n","\n","      prompt = \"\"\n","      for tuple1 in reOrderPromptSample:\n","        id, Type, tweet, explanation = tuple1\n","        prompt += \"Tweet: \" + tweet + \"\\n \" + question + explanation + \"\\n\"\n","\n","      tempPrompt = prompt + \"Tweet: \" + speech + '\\n' + question\n","      run = openai.Completion.create(\n","        model = \"text-davinci-002\",\n","        prompt= tempPrompt,\n","        temperature=0.6,\n","        max_tokens=1000, \n","        n= 1,\n","      )\n","      exp = run.choices[0]['text']\n","\n","      exp_lis.append([hateType ,speech, exp])\n","  df = pd.DataFrame(exp_lis) \n","  df.to_csv(file_path, header= [ \"Type\", \"Tweet\", \"Explanation\"]) \n","\n","# Denerate explanations for each demonstration tweets and store them into files.\n","def store_evaluation_twt_exp(tweets_dic, question, demo_path, evaluation_path):\n","    data = pd.read_csv(demo_path)\n","    demo_list = []\n","    for i in range(len(data[\"ID\"])):\n","      twt = data[\"Tweet\"][i]\n","      typ =  data[\"Type\"][i]\n","      exp = data[\"Explanation\"][i]\n","      demo_list.append( [i, twt, typ, exp] )\n","    \n","    evaluation_tweets_exp_path = root + \"demo/\" + key + \"_evaluation_tweets_exp.csv\"\n","    few_shots_exp_generation(tweets_dic, demo_list, question, evaluation_path)\n","\n","\n","# For WHY and COT prompting strategy, generate both-WHY and both-COT by combining both hateful and non-hateful explanations.\n","def generate_head_to_head_explanations(hateful_evaluation_explanation_path, non_hateful_evaluation_explanation_path, both_evaluation_explanation_path):\n","  data1 = pd.read_csv(hateful_evaluation_explanation_path)\n","  data2 = pd.read_csv(non_hateful_evaluation_explanation_path)\n","\n","  exp_list = []\n","  for i in range(len(data1[\"Explanation\"])):\n","    typ =  data1[\"Type\"][i]\n","    twt = data1[\"Tweet\"][i]\n","    exp1 = data1[\"Explanation\"][i]\n","    exp2 = data2[\"Explanation\"][i]\n","    exp = \"Explanation1: \\n\" + exp1 + \"\\n\" + \"Explanation2: \\n\" + exp2\n","    exp_list.append([typ, twt, exp])\n","  df = pd.DataFrame(exp_list) \n","  df.to_csv(both_evaluation_explanation_path, header= [ \"Type\", \"Tweet\", \"Explanation\"]) \n"],"metadata":{"id":"6W7dpJcfxFss"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for key in Question_dic:\n","  selected_demostration_example_path = demostration_example_path + key + \"_demonstration_examples_selected.csv\"\n","  current_evaluation_explanation_path = evaluation_explanation_path + key + \".csv\"\n","  store_evaluation_twt_exp(samplePool, Question_dic[key], selected_demostration_example_path, current_evaluation_explanation_path)\n"],"metadata":{"id":"gCssQFdPW-c8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["both_list = [\"COT\", \"WHY\"]\n","for ket in both_list:\n","  current_evaluation_explanation_path1 = evaluation_explanation_path + \"hateful-\" + key + \".csv\"\n","  current_evaluation_explanation_path2 = evaluation_explanation_path + \"non-hateful-\" + key + \".csv\"\n","  current_evaluation_explanation_path3 = evaluation_explanation_path + \"both-\" + key + \".csv\"\n","  generate_head_to_head_explanations(current_evaluation_explanation_path1, current_evaluation_explanation_path2, current_evaluation_explanation_path3)"],"metadata":{"id":"-rIDJTcRl5_z"},"execution_count":null,"outputs":[]}]}